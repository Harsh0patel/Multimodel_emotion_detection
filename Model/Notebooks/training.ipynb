{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8234a23",
   "metadata": {},
   "source": [
    "# 1) Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef89cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d26140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "\n",
    "# Define paths\n",
    "DATA_DIRS = {\n",
    "    \"dev\": \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/MELD.Raw/dev/dev_splits_complete\",\n",
    "    \"test\": \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/MELD.Raw/test/output_repeated_splits_test\",\n",
    "    \"train\": \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/MELD.Raw/train/train_splits\"\n",
    "}\n",
    "OUTPUT_BASE = \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/Model/Notebooks/meld_audio\"\n",
    "\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "bad_files = []\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def extract_audio(video_path, out_path):\n",
    "    try:\n",
    "        cmd = [\n",
    "            \"ffmpeg\", \"-y\",\n",
    "            \"-i\", video_path,\n",
    "            \"-vn\",\n",
    "            \"-ac\", \"1\",\n",
    "            \"-ar\", \"16000\",\n",
    "            \"-acodec\", \"pcm_s16le\",\n",
    "            out_path\n",
    "        ]\n",
    "        subprocess.run(\n",
    "            cmd,\n",
    "            check=True,\n",
    "            stdout=subprocess.DEVNULL,\n",
    "            stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        bad_files.append((video_path, str(e)))\n",
    "        return False\n",
    "\n",
    "\n",
    "for split, in_dir in DATA_DIRS.items():\n",
    "    out_dir = os.path.join(OUTPUT_BASE, split)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    video_files = [\n",
    "    f for f in os.listdir(in_dir)\n",
    "    if f.endswith(\".mp4\") and not f.startswith(\"._\")]\n",
    "    print(f\"\\nğŸ¬ Processing {len(video_files)} files in {split} split...\")\n",
    "    \n",
    "    success, fail = 0, 0\n",
    "    for f in tqdm(video_files):\n",
    "        in_path = os.path.join(in_dir, f)\n",
    "        out_path = os.path.join(out_dir, f.replace(\".mp4\", \".wav\"))\n",
    "        if extract_audio(in_path, out_path):\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "\n",
    "    print(f\"âœ… {success} extracted | âŒ {fail} failed in {split} split.\")\n",
    "\n",
    "print(\"\\nğŸš¨ Bad files logged:\")\n",
    "for f, err in bad_files[:10]:\n",
    "    print(f\"- {f}: {err}\")\n",
    "print(f\"\\nTotal bad files: {len(bad_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1b5eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp333\\Desktop\\Multimodel_emotion_detection\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb08a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_AUDIO_LEN = 16000 * 5  # 5 sec clips\n",
    "MAX_TEXT_LEN = 64\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "LR = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac93daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "text_model_name = \"distilbert-base-uncased\"\n",
    "audio_model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_encoder = AutoModel.from_pretrained(text_model_name).to(DEVICE)\n",
    "\n",
    "audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
    "audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_name).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56413263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_np(audio_array, target_len):\n",
    "    \"\"\"Ensure audio array is exactly target_len long.\"\"\"\n",
    "    if len(audio_array) > target_len:\n",
    "        return audio_array[:target_len]\n",
    "    elif len(audio_array) < target_len:\n",
    "        return np.pad(audio_array, (0, target_len - len(audio_array)))\n",
    "    return audio_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fa3504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MELDFusionDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, tokenizer, feature_extractor, label_encoder, max_text_len=64):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_text_len = max_text_len\n",
    "        self.df = self.df[self.df['Utterance'].notnull()]  # clean missing text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        utt = row[\"Utterance\"]\n",
    "        label = self.label_encoder.transform([row[\"Emotion\"]])[0]\n",
    "        dia, utt_id = row[\"Dialogue_ID\"], row[\"Utterance_ID\"]\n",
    "        audio_path = os.path.join(self.audio_dir, f\"dia{dia}_utt{utt_id}.wav\")\n",
    "\n",
    "        # --- TEXT ---\n",
    "        text_inputs = self.tokenizer(\n",
    "            utt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # --- AUDIO ---\n",
    "        if os.path.exists(audio_path):\n",
    "            try:\n",
    "                audio, _ = librosa.load(audio_path, sr=16000)\n",
    "                audio = pad_or_truncate_np(audio, MAX_AUDIO_LEN)\n",
    "            except Exception:\n",
    "                audio = np.zeros(MAX_AUDIO_LEN)\n",
    "        else:\n",
    "            audio = np.zeros(MAX_AUDIO_LEN)\n",
    "\n",
    "        # âœ… Explicitly wrap in list to force feature extractor padding uniformity\n",
    "        audio_inputs = self.feature_extractor(\n",
    "            [audio], sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text_inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": text_inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"audio_values\": audio_inputs[\"input_values\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f420d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, text_dim=768, audio_dim=768, hidden_dim=512, num_classes=7):\n",
    "        super().__init__()\n",
    "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
    "        self.audio_proj = nn.Linear(audio_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "\n",
    "    def forward(self, text_emb, audio_emb):\n",
    "        t = torch.relu(self.text_proj(text_emb))\n",
    "        a = torch.relu(self.audio_proj(audio_emb))\n",
    "        fused = torch.cat([t, a], dim=1)\n",
    "        fused = self.dropout(fused)\n",
    "        return self.fc(fused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbfc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/MELD.Raw/train/train_sent_emo.csv\"\n",
    "dev_csv = \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/MELD.Raw/dev/dev_sent_emo.csv\"\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_df = pd.read_csv(train_csv)\n",
    "le.fit(train_df[\"Emotion\"])\n",
    "\n",
    "train_set = MELDFusionDataset(train_csv, \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/meld_audio/train\", text_tokenizer, audio_extractor, le)\n",
    "dev_set = MELDFusionDataset(dev_csv, \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/data/meld_audio/dev\", text_tokenizer, audio_extractor, le)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5b3e590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_embeddings(input_ids, attention_mask):\n",
    "    with torch.no_grad():\n",
    "        outputs = text_encoder(input_ids.to(DEVICE), attention_mask=attention_mask.to(DEVICE))\n",
    "        return outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "\n",
    "def compute_audio_embeddings(audio_values):\n",
    "    with torch.no_grad():\n",
    "        outputs = audio_encoder(audio_values.to(DEVICE))\n",
    "        return outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bffa799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, epoch):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader,desc=f\"Epoch {epoch+1}/{EPOCHS} [Dev]\"):\n",
    "            # move batch to device\n",
    "            batch = {k: v.to(DEVICE) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "\n",
    "            text_emb = compute_text_embeddings(\n",
    "                batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "            )\n",
    "            audio_emb = compute_audio_embeddings(\n",
    "                batch[\"audio_values\"]\n",
    "            )\n",
    "\n",
    "            logits = model(text_emb, audio_emb)\n",
    "            loss = criterion(logits, batch[\"labels\"])\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    return total_loss / len(data_loader), acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59b4ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [09:30<00:00,  2.19it/s]\n",
      "Epoch 1/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [01:01<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 1: Train Loss=1.5016, Acc=0.4702, F1=0.3110 | Dev Loss=1.5044, Acc=0.4418, F1=0.2903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:47<00:00,  2.37it/s]\n",
      "Epoch 2/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:57<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 2: Train Loss=1.3527, Acc=0.5307, F1=0.4182 | Dev Loss=1.4085, Acc=0.5095, F1=0.3997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:45<00:00,  2.38it/s]\n",
      "Epoch 3/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 3: Train Loss=1.2881, Acc=0.5656, F1=0.4794 | Dev Loss=1.3594, Acc=0.5230, F1=0.4274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:50<00:00,  2.35it/s]\n",
      "Epoch 4/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 4: Train Loss=1.2529, Acc=0.5774, F1=0.5026 | Dev Loss=1.3327, Acc=0.5392, F1=0.4634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:51<00:00,  2.35it/s]\n",
      "Epoch 5/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 5: Train Loss=1.2284, Acc=0.5866, F1=0.5196 | Dev Loss=1.3101, Acc=0.5446, F1=0.4731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:50<00:00,  2.36it/s]\n",
      "Epoch 6/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 6: Train Loss=1.2090, Acc=0.5915, F1=0.5292 | Dev Loss=1.3027, Acc=0.5419, F1=0.4710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:53<00:00,  2.34it/s]\n",
      "Epoch 7/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 7: Train Loss=1.1954, Acc=0.5961, F1=0.5377 | Dev Loss=1.2858, Acc=0.5518, F1=0.4887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:47<00:00,  2.37it/s]\n",
      "Epoch 8/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:57<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 8: Train Loss=1.1873, Acc=0.6004, F1=0.5451 | Dev Loss=1.2957, Acc=0.5464, F1=0.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:47<00:00,  2.37it/s]\n",
      "Epoch 9/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 9: Train Loss=1.1769, Acc=0.6049, F1=0.5505 | Dev Loss=1.2730, Acc=0.5537, F1=0.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:52<00:00,  2.35it/s]\n",
      "Epoch 10/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:57<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 10: Train Loss=1.1666, Acc=0.6088, F1=0.5572 | Dev Loss=1.2755, Acc=0.5546, F1=0.4860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:50<00:00,  2.36it/s]\n",
      "Epoch 11/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:59<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 11: Train Loss=1.1633, Acc=0.6082, F1=0.5572 | Dev Loss=1.2574, Acc=0.5618, F1=0.5045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:52<00:00,  2.34it/s]\n",
      "Epoch 12/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:59<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 12: Train Loss=1.1596, Acc=0.6093, F1=0.5599 | Dev Loss=1.2544, Acc=0.5618, F1=0.5032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:49<00:00,  2.36it/s]\n",
      "Epoch 13/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:58<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 13: Train Loss=1.1519, Acc=0.6149, F1=0.5663 | Dev Loss=1.2548, Acc=0.5627, F1=0.5015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:51<00:00,  2.35it/s]\n",
      "Epoch 14/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:59<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 14: Train Loss=1.1476, Acc=0.6151, F1=0.5678 | Dev Loss=1.2509, Acc=0.5627, F1=0.5028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1249/1249 [08:48<00:00,  2.36it/s]\n",
      "Epoch 15/15 [Dev]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139/139 [00:57<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Checkpoint saved (can resume training)\n",
      "Epoch 15: Train Loss=1.1418, Acc=0.6131, F1=0.5658 | Dev Loss=1.2566, Acc=0.5627, F1=0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = FusionClassifier().to(DEVICE)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "SAVE_DIR = \"C:/Users/hp333/Desktop/Multimodel_emotion_detection/Model/checkpoints\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "checkpoint_path = os.path.join(SAVE_DIR, \"fusion_checkpoint.pt\")\n",
    "\n",
    "\n",
    "start_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_dev_f1 = checkpoint[\"best_dev_f1\"]\n",
    "    print(f\"âœ… Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    y_true, y_pred = [], []\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        optimizer.zero_grad()\n",
    "        text_emb = compute_text_embeddings(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "        audio_emb = compute_audio_embeddings(batch[\"audio_values\"])\n",
    "        logits = model(text_emb, audio_emb)\n",
    "        loss = criterion(logits, batch[\"labels\"].to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).detach().cpu().numpy()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(batch[\"labels\"].cpu().numpy())\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    dev_loss, dev_acc, dev_f1 = evaluate(model, dev_loader, epoch)\n",
    "\n",
    "    checkpoint_path = os.path.join(SAVE_DIR, \"fusion_checkpoint.pt\")\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"best_dev_f1\": dev_f1\n",
    "    }, checkpoint_path)\n",
    "\n",
    "    print(\"ğŸ’¾ Checkpoint saved (can resume training)\")\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}: \"\n",
    "        f\"Train Loss={total_loss/ len(train_loader):.4f}, Acc={acc:.4f}, F1={f1:.4f} | \"\n",
    "        f\"Dev Loss={dev_loss:.4f}, Acc={dev_acc:.4f}, F1={dev_f1:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
